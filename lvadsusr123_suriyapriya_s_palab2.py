# -*- coding: utf-8 -*-
"""LVADSUSR123_Suriyapriya S_PALab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tkf264Te4-8t3TfNvktc-KiWaaOoI5rB
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

data = pd.read_csv('/content/drive/MyDrive/booking.csv')

#qn1
print('Number of missing values in each column:')
print(data.isnull().sum())

data.dropna(inplace=True)

numerical_cols = ['number of adults', 'number of children', 'number of weekend nights', 'number of week nights','lead time','average price','special requests']
plt.figure(figsize=(10, 6))
data[numerical_cols].boxplot()
plt.title('Box plot of numerical columns')
plt.xticks(rotation=45)
plt.show()

Q1 = data['average price'].quantile(0.25)
Q3 = data['average price'].quantile(0.75)
IQR = Q3 - Q1
data = data[~((data['average price'] < (Q1 - 1.5 * IQR)) | (data['average price'] > (Q3 + 1.5 * IQR)))]

Q1 = data['lead time'].quantile(0.25)
Q3 = data['lead time'].quantile(0.75)
IQR = Q3 - Q1
data = data[~((data['lead time'] < (Q1 - 1.5 * IQR)) | (data['lead time'] > (Q3 + 1.5 * IQR)))]

#qn2
data = pd.get_dummies(data, columns=['room type', 'type of meal', 'market segment type'])

#qn3
data.drop_duplicates(inplace=True)
data.drop(columns=['Booking_ID', 'date of reservation'], inplace=True)

#qn4
X = data.drop(columns=['booking status'])
y = data['booking status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#qn5

#Logistic
log_reg_model = LogisticRegression()
log_reg_model.fit(X_train, y_train)
y_pred = log_reg_model.predict(X_test)

#DecisionTree
tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)
tree_pred = tree_model.predict(X_test)

#qn6

#Logistic regression
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
confusion_matrix_logistic = confusion_matrix(y_test, y_pred)

print('Logistic regression Model Evaluation:')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1-score:', f1)
print('Confusion Matrix:')
print(confusion_matrix_logistic)

#Decision Tree

tree_accuracy = accuracy_score(y_test, tree_pred)
precision_tree = precision_score(y_test, tree_pred, average='weighted')
recall_tree = recall_score(y_test, tree_pred, average='weighted')
f1_tree = f1_score(y_test, tree_pred, average='weighted')
confusion_matrix_tree = confusion_matrix(y_test, tree_pred)

print("Decision Tree Model Evaluation:")
print("Accuracy:", tree_accuracy)
print('Precision:', precision_tree)
print('Recall:', recall_tree)
print('F1-score:', f1_tree)
print("Confusion Matrix:")
print(confusion_matrix_tree)

# Classification Report
print("\nLogistic Regression Classification Report:")
print(classification_report(y_test, y_pred))

print("\nDecision Tree Classification Report:")
print(classification_report(y_test, tree_pred))

"""The Sigmoid function is a mathematical curve that maps any real number to a value between 0 and 1. In logistic regression, it converts the linear combination of features and coefficients to a probability score. This probability score represents the likelihood that a given input belongs to the positive class. If the probability is greater than 0.5, the model predicts a positive class; otherwise, it predicts a negative class.

The Sigmoid function helps the logistic regression model to make binary classification by providing a probability-based interpretation of the model's output.
"""