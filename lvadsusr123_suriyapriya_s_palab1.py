# -*- coding: utf-8 -*-
"""LVADSUSR123_Suriyapriya S_PALab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1te8RK7egmYPLz0eUaDZfH_6Jll8d3wZy
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

data = pd.read_csv('/content/drive/MyDrive/expenses.csv')

#qn1
print("Number of missing values in each column:")
print(data.isnull().sum())

data.dropna(inplace=True)

numerical_cols = ['age', 'bmi', 'children', 'charges']
plt.figure(figsize=(10, 6))
data[numerical_cols].boxplot()
plt.title('Box plot of numerical columns')
plt.xticks(rotation=45)
plt.show()

data = data[data['charges'] < 50000]

#qn2
data = pd.get_dummies(data, columns=['sex', 'smoker', 'region'])

#qn3
data.drop_duplicates(inplace=True)

#qn4
X = data.drop(columns=['charges'])
y = data['charges']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#qn5
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

y_pred = lr_model.predict(X_test)

#qn6
mse = mean_squared_error(y_test, y_pred)
r_squared = r2_score(y_test, y_pred)
rmse = mse ** 0.5

print("Model Evaluation:")
print("Mean Squared Error:", mse)
print("R-squared:", r_squared)
print("Root Mean Squared Error:", rmse)

plt.title('Distribution of insurance charges')
sns.histplot(data['charges'], bins=20, kde=True)

"""In Gradient Descent, the Learning Rate determines the size of steps taken towards minimizing the cost function. Very high learning rate leads to overshooting, and very small learning rate may lead to slow convergence.

Derivatives and partial derivatives indicate the slope of the cost function. It guides the algorithm towards a steep descent. They play an important role in determining the direction and magnitude of parameter, and it helps to optimize the model and improve its performance.
"""

